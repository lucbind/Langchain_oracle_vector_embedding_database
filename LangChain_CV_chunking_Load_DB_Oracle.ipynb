{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"chunking.png\" width=\"200\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Chunking PDF Docuemnts, Load in dataabse Prompt RAG question \n",
    "sono configurati diversi metodi di chunking riferiemnto \n",
    "https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a\n",
    "\n",
    "* Spacy Sentence Splitter \n",
    "* Langchain Character Text Splitter  \n",
    "* NLTK Sentence Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pacchetti da installare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.0.349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install  sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install oracledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install /Users/LBINDI/Downloads/p36050269_230000_Linux-x86-64/drivers/oracledb-2.0.0.dev20231121-cp311-cp311-macosx_10_9_universal2.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine pacchetti da installare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python librerie Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import oracledb\n",
    "import cohere\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartella con pdf dei CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartella = '/Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY=\"kKC103gzFwcLvsGfSNeXZsuN38BHrBAelzgefAez\"\n",
    "co = cohere.Client(CO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your Oracle 23.4 database\n",
    "connection = oracledb.connect(\n",
    "    user=\"vector\",\n",
    "    password=\"vector\",\n",
    "    dsn=\"158.101.175.18:1521/freepdb1\"\n",
    "    )\n",
    "cursor = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop and create table Vector_CV_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    begin\n",
    "        execute immediate 'drop table Vector_CV_PDF';\n",
    "        exception when others then if sqlcode <> -942 then raise; end if;\n",
    "    end;\"\"\")\n",
    "# Create table Vector_TEST1\n",
    "cursor.execute(\"\"\"\n",
    "    create table Vector_CV_PDF (\n",
    "        id_pdf number,\n",
    "        id_chunk number,\n",
    "        nome_file varchar2(300),       \n",
    "        v vector(1024, float32),\n",
    "        chunk varchar2(4000),\n",
    "        primary key (id_pdf,id_chunk))\"\"\")\n",
    "\n",
    "#v vector(384, float32) --> 384 e' la dimensione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per estrarre i file PDF  all'interno della cartella fornita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elenca_files_con_percorsi(carta):\n",
    "    lista_files = []\n",
    "    for cartella_radice, sottocartelle, files in os.walk(carta):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):  # Verifica se il file ha estensione PDF\n",
    "                  percorso_completo = os.path.join(cartella_radice, file)\n",
    "                  lista_files.append(percorso_completo)\n",
    "    return lista_files\n",
    "#print (len(lista_file))\n",
    "#print (lista_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per convertire pdf to testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting Text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "Funzione per suddivisione in chunk mediante spacy del testo , tennde a creare piccoli chiuck in confronto con  Langchain Character Text Splitter, . Puo essere vantaggioso con piccoli testi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuk Text using spacy \n",
    "def divide_in_frase(testo):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(testo)\n",
    "    frasi = list(doc.sents)\n",
    "    return frasi\n",
    "#testo_da_dividere = \"Questo è un esempio di testo. Può contenere più di una frase. SpaCy è uno strumento di elaborazione del linguaggio naturale.\"\n",
    "#frasi_divise = divide_in_frase(testo_da_dividere)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Character Text Splitter\n",
    "RecursiveCharacterTextSplitter ricorsivamente divide il testo sulla base di specifici caratteri .\n",
    "Puo essere vantaggioso con Testi generici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_text_splitter(text, chunk_size=100, chunk_overlap=10):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.create_documents([text])\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Sentence Tokenizer\n",
    "Il Natural Language Toolkit (NLTK) fornisce una funzione utile per suddividere il testo in frasi. e' un modo semplice ed efficiente per dividere un ampio corpo di testo in singole frasi,ma ha alcune limitazioni:\n",
    "* Dipendenza dalla lingua:Funziona bene con l'inglese ma potrebbe non fornire risultati accurati con altre lingue senza una configurazione aggiuntiva.\n",
    "* Abbreviazioni e punteggiatura: il tokenizzatore può occasionalmente interpretare erroneamente le abbreviazioni o altri segni di punteggiatura.\n",
    "* Mancanza di comprensione semantica: come la maggior parte dei tokenizzatori, il tokenizzatore di frasi NLTK non considera la relazione semantica tra le frasi. Pertanto, un contesto che si estende su più frasi potrebbe andare perso nel processo di tokenizzazione.\n",
    "\n",
    "* nltk.punkt è un modulo di NLTK (Natural Language Toolkit) che fornisce un tokenizer di frasi. Il tokenizer divide un testo in una lista di frasi utilizzando un algoritmo non supervisionato per costruire un modello per le parole di abbreviazione, le collocazioni e le parole che iniziano le frasi.\n",
    "* nltk.corpus: fornisce accesso a una vasta gamma di corpora etichettati e non etichettati, come il corpus Brown, il corpus Penn Treebank, il corpus WordNet e molti altri.\n",
    "* nltk.tokenize: fornisce una serie di tokenizzatori per la suddivisione del testo in parole, frasi e altre unità.\n",
    "* nltk.stem: fornisce una serie di algoritmi di stemming per ridurre le parole alle loro forme di base.\n",
    "* nltk.tag: fornisce una serie di modelli di tagging per l’etichettatura delle parti del discorso, l’etichettatura delle entità denominate e altro ancora.\n",
    "* nltk.chunk: fornisce una serie di modelli di chunking per l’identificazione di frasi nominali, verbi e altre unità sintattiche.\n",
    "* nltk.parse: fornisce una serie di parser per l’analisi sintattica del testo.\n",
    "* nltk.sentiment: fornisce una serie di modelli per l’analisi del sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confronta i diversi metodi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file :  0\n",
      "Spacy Sentence Splitter num chunk  17\n",
      "* Langchain Character Text Splitter num chunk  15\n",
      "NLTK Sentence Tokenizer num chunk  15\n",
      "['First Name: Luca Last Name: Migliori Address: Mecenati Street, Florence 00302 Profile: Machine Learning expert with a strong background in designing and implementing advanced models.', 'Specialized in Python, TensorFlow, and PyTorch, I take a practical approach to data analysis and solving complex problems.', 'Experience: University researcher, where I optimized existing models and integrated machine learning solutions into business processes.', 'Previously worked at the university, focusing on specific algorithms for the military sector.', 'Key Skills: Excellent proficiency in Python, TensorFlow, and PyTorch for developing machine learning models.', 'Established experience in regression, classification, and clustering algorithms.', 'Advanced skills in NLP with BERT and GPT.', \"Education: Master's degree in Computer Engineering from the University of Florence.\", 'Relevant Projects: Development of a speech recognition system based on neural networks.', 'Implementation of an NLP model for the automatic classification of business documents.', 'Certifications: TensorFlow certified, 2016.', 'Publications: Machine learning and vectors, 2019.', 'Extracurricular Activities: Participation in conferences and workshops in the field of machine learning.', 'Contributions to open-source projects.', 'References: Available upon request.']\n",
      "file :  1\n",
      "Spacy Sentence Splitter num chunk  19\n",
      "* Langchain Character Text Splitter num chunk  22\n",
      "NLTK Sentence Tokenizer num chunk  16\n",
      "['first_name :  Pino Last Name: Sarsi Personal Summary:born in 1990 and is grow up in milano.', 'Professional Summary: Experienced programmer with a strong proficiency in crafting efficient and reliable code.', 'Proven expertise in designing and implementing software solutions with a focus on code quality and application performance.', 'Excellent problem-solving skills and quick adaptability to new technologies.', 'Work Experience:  Senior Developer | oracle, roma | 2022 - Present: 2 years   Designed and implemented software solutions to enhance system efficiency and functionality.', 'Collaborated with the development team to define and achieve project goals.', 'Optimized code performance and efficiently resolved bugs.', 'Software Developer | telecom, napoli 2006 - 2022:  Contributed to the development of software applications, ensuring compliance with quality standards.', 'Collaborated with other departments to understand requirements and provide appropriate technical solutions.', 'Actively participated in code reviews to ensure software integrity and consistency.', \"Education:  Bachelor's Degree in Computer Science | roma torvergata University , roma  Technical Skills:  Programming Languages: Python, Java, C++ Web Development: HTML, CSS, JavaScript, Frameworks (e.g., Django, Flask) Databases: SQL, MongoDB Version Control Tools: Git Agile Methodologies: Scrum, Kanban Operating Systems: Linux, Windows Relevant Projects:  Developed a web application for efficient task management within the company.\", 'Created an automation system to improve team productivity.', 'Certifications:  Certified in coding, 2000 Languages:  Native Language: italiano Additional Languages: English Hobbies and Interests:  Participation in hackathons and coding competitions.', 'Exploration of new technologies and programming languages.', 'Contributions to open-source projects.', 'References: Available upon request.']\n",
      "file :  2\n",
      "Spacy Sentence Splitter num chunk  15\n",
      "* Langchain Character Text Splitter num chunk  28\n",
      "NLTK Sentence Tokenizer num chunk  14\n",
      "['first_name :  Paolo Last Name:Cecchi Professional Summary: Experienced Cloud Interconnect Specialist with a proven track record of designing and implementing robust cloud connectivity solutions.', 'Proven expertise in optimizing network performance and ensuring seamless integration of cloud services.', 'Strong analytical and problem-solving skills with a focus on delivering high-quality solutions.', 'Work Experience:  Cloud Interconnect Specialist | SkyLink Solutions, New York | January 2018 - Present:  Spearheaded the design and implementation of scalable and secure cloud interconnect solutions for multinational clients.', 'Collaborated with cross-functional teams to analyze complex network requirements and delivered customized connectivity solutions.', 'Achieved notable success in optimizing network performance, resulting in a 20% reduction in data transfer latency.', 'Network Engineer | GlobalTel Communications, San Francisco | June 2015 - December 2017:  Configured and maintained a sophisticated network infrastructure, specializing in cloud connectivity solutions.', 'Implemented redundant and fault-tolerant solutions, ensuring a 99.99% service uptime.', 'Conducted in-depth performance analysis, leading to the implementation of strategic optimizations for enhanced network efficiency.', 'Education:  Master of Science in Network Engineering | Tech Institute, Silicon Valley | Graduated in 2015  Technical Skills:  Cloud Platforms: AWS, Azure, Google Cloud Cloud Interconnect Technologies: Direct Connect, ExpressRoute, Interconnect Networking Protocols: TCP/IP, BGP, OSPF Security: VPNs, Firewalls Network Monitoring Tools: Wireshark, Nagios Programming: Python, Bash Relevant Projects:  Successfully implemented a hybrid cloud architecture, seamlessly integrating on-premises and cloud resources.', 'Led the enhancement of network security through the implementation of VPN solutions for remote access.', 'Certifications:  Certified Cloud Network Engineer (CCNE)  AWS Certified Solutions Architect - Professional Azure Solutions Architect Expert Languages:  Native Language: English Additional Languages: Spanish Hobbies and Interests:  Continuous learning about emerging trends in cloud networking.', 'Active participation in industry forums and conferences.', 'References:  Dr. Eleanor Mitchell Cloud Networking Professor Tech Institute, Silicon Valley Email: e.mitchell@example.com Phone: (555) 987-6543  Mr. James Reynolds Cloud Services Director SkyLink Solutions, New York Email: j.reynolds@example.com Phone: (555) 123-4567']\n",
      "file :  3\n",
      "Spacy Sentence Splitter num chunk  19\n",
      "* Langchain Character Text Splitter num chunk  22\n",
      "NLTK Sentence Tokenizer num chunk  16\n",
      "['first_name :  Eugenio Last Name: Bernaci Personal Summary:born in 1970 and is grow up in rome.', 'Professional Summary: Experienced programmer with a strong proficiency in crafting efficient and reliable code.', 'Proven expertise in designing and implementing software solutions with a focus on code quality and application performance.', 'Excellent problem-solving skills and quick adaptability to new technologies.', 'Work Experience:  Senior Developer | oracle, roma | 2022 - Present: 2 years   Designed and implemented software solutions to enhance system efficiency and functionality.', 'Collaborated with the development team to define and achieve project goals.', 'Optimized code performance and efficiently resolved bugs.', 'Software Developer | telecom, napoli 2006 - 2022:  Contributed to the development of software applications, ensuring compliance with quality standards.', 'Collaborated with other departments to understand requirements and provide appropriate technical solutions.', 'Actively participated in code reviews to ensure software integrity and consistency.', \"Education:  Bachelor's Degree in Computer Science | roma torvergata University , roma  Technical Skills:  Programming Languages: Python, Java, C++ Web Development: HTML, CSS, JavaScript, Frameworks (e.g., Django, Flask) Databases: SQL, MongoDB Version Control Tools: Git Agile Methodologies: Scrum, Kanban Operating Systems: Linux, Windows Relevant Projects:  Developed a web application for efficient task management within the company.\", 'Created an automation system to improve team productivity.', 'Certifications:  Certified in coding, 2000 Languages:  Native Language: italiano Additional Languages: English Hobbies and Interests:  Participation in hackathons and coding competitions.', 'Exploration of new technologies and programming languages.', 'Contributions to open-source projects.', 'References: Available upon request.']\n",
      "file :  4\n",
      "Spacy Sentence Splitter num chunk  14\n",
      "* Langchain Character Text Splitter num chunk  24\n",
      "NLTK Sentence Tokenizer num chunk  13\n",
      "['first_name :  Mario Last Name: Bianchi  Professional Summary: Dynamic and results-oriented Enterprise Technology Architect with a wealth of experience in crafting and implementing cutting-edge solutions within the Oracle ecosystem.', 'Demonstrated ability to align technological strategies with organizational objectives, driving innovation and efficiency.', 'Exceptional leadership and communication skills.', 'Work Experience:  Enterprise Technology Architect | Oracle Corporation, Silicon Valley | January 2018 - Present:  Spearhead the design and implementation of Oracle-based enterprise solutions for clients across diverse industries.', 'Collaborate with cross-functional teams to analyze business requirements and develop scalable architecture solutions.', 'Leverage expertise in Oracle Cloud Infrastructure, Database, and Fusion Middleware to optimize system performance.', 'Successfully executed multiple large-scale projects, resulting in heightened operational efficiency and reduced costs.', 'Conduct workshops and training sessions to elevate the technical proficiency of both the team and clients.', 'Senior Solutions Architect | XYZ Tech Solutions, Citytown | June 2015 - December 2017:  Designed and implemented Oracle-based solutions, ensuring alignment with industry best practices.', 'Collaborated with sales teams to discern client needs and proposed tailored solutions meeting business requirements.', 'Led the seamless migration of legacy systems to Oracle Cloud, enhancing scalability and reducing maintenance costs.', 'Provided technical leadership in the evaluation and adoption of emerging Oracle technologies.', 'Education:  Bachelor of Science in Computer Science | Tech University, Citytown | May 2015  Certifications:  Oracle Certified Master, Oracle Cloud Infrastructure Architect Oracle Certified Professional, Database Administrator Technical Skills:  Oracle Cloud Infrastructure Oracle Database Oracle Fusion Middleware Oracle Exadata Oracle Enterprise Manager  Java and PL/SQL programming System Integration Professional Memberships:  Member, Oracle ACE Program Member, Oracle User Group Languages:  Fluent in English and Spanish References:Available upon request']\n"
     ]
    }
   ],
   "source": [
    "id_file=0\n",
    "lista_file= elenca_files_con_percorsi(cartella)\n",
    "for nume_file in range(len(lista_file)):\n",
    "    id_file+=1\n",
    "    id_chunk=0\n",
    "    # print (id_nome_file)\n",
    "    # Extract text from the PDF and split it into sentences\n",
    "    nome_file=lista_file[nume_file]\n",
    "    text = extract_text_from_pdf(lista_file[nume_file])\n",
    "    #print (text)\n",
    "    spacy_frasi_divise = divide_in_frase(text)\n",
    "    splitter_result_chunks = custom_text_splitter(text)\n",
    "    nltk_sentences = split_text_into_sentences(text)\n",
    "    print ('file : ', nume_file)\n",
    "    print ('Spacy Sentence Splitter num chunk ',len(spacy_frasi_divise))\n",
    "    print ('* Langchain Character Text Splitter num chunk ',len(splitter_result_chunks))\n",
    "    print ('NLTK Sentence Tokenizer num chunk ',len(nltk_sentences))\n",
    "\n",
    "   # print (spacy_frasi_divise)\n",
    "   # print (splitter_result_chunks)\n",
    "   # print (splitter_result_chunks[0].page_content)\n",
    "   # print (nltk_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Implementazione\n",
    " Conversione  di tutti i pdf, della cartella fornita, in text, successivamente suddiviso il testo  in chuck mediante spacy e traformati in vector embedding con Cohere embed , alla fine inseriti all'interno della tabella vector_PDF del database Oracel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero di chunk per il pdf  :  /Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF/luca_Migliori.pdf  --  15\n",
      "numero di chunk per il pdf  :  /Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF/pino_sarsi.pdf  --  16\n",
      "numero di chunk per il pdf  :  /Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF/Paolo_checchi.pdf  --  14\n",
      "numero di chunk per il pdf  :  /Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF/eugenio_bernaci.pdf  --  16\n",
      "numero di chunk per il pdf  :  /Users/LBINDI/GreenHouse/Documents/LB/Cloud/LAB_ORDS/Embedding/docs/CV_PDF/mario_bianchi.pdf  --  13\n",
      "numero documenti pdf totali inseriti :  5\n"
     ]
    }
   ],
   "source": [
    "id_file=0\n",
    "cursor.setinputsizes(None,None,oracledb.DB_TYPE_VARCHAR, oracledb.DB_TYPE_VECTOR )\n",
    "cohere_model_def='embed-english-v3.0'\n",
    "cohere_input_type='search_document'\n",
    "lista_file= elenca_files_con_percorsi(cartella)\n",
    "for nume_file in range(len(lista_file)):\n",
    "    id_file+=1\n",
    "    id_chunk=0\n",
    "    # print (id_nome_file)\n",
    "    # Extract text from the PDF and split it into sentences\n",
    "    nome_file=lista_file[nume_file]\n",
    "    text = extract_text_from_pdf(lista_file[nume_file])\n",
    "    #print (text)\n",
    "    #scegliere quale metodo di suddivisione testo utilizzare\n",
    "    #\n",
    "    #frasi_divise = spacy_frasi_divise = divide_in_frase(text)\n",
    "    #frasi_divise = splitter_result_chunks = custom_text_splitter(text)\n",
    "    frasi_divise = nltk_sentences = split_text_into_sentences(text)\n",
    "    #\n",
    "    api_call=0\n",
    "    for frase in frasi_divise:\n",
    "        api_call+=1\n",
    "        if api_call<90:\n",
    "            id_chunk+=1\n",
    "            response = co.embed(\n",
    "                    #spacy_frasi_divise\n",
    "                    #texts=[frase.text],\n",
    "                    #nltk_sentences\n",
    "                    texts=[frase],\n",
    "                    #splitter_result_chunks\n",
    "                    #texts=[frase non provato non frase[0].page_content\n",
    "                    model=cohere_model_def,\n",
    "                    input_type=cohere_input_type\n",
    "            )\n",
    "            #print (response)\n",
    "            vector_result = response.embeddings[0]\n",
    "            #print (vector_result)\n",
    "            ##print(id_file,nome_file,vector_result,frase)\n",
    "            # Insert the vector using the bind variable from the generated vector\n",
    "            cursor.execute(\"insert into Vector_CV_PDF values (:1,:2,:3,:4,:5)\", [id_file,id_chunk,nome_file, vector_result,frase])\n",
    "        else :\n",
    "            api_call=0\n",
    "            time.sleep(60)\n",
    "    print ( 'numero di chunk per il pdf  : ' , nome_file, ' -- ',id_chunk)\n",
    "    connection.commit()\n",
    "\n",
    "print ( 'numero documenti pdf totali inseriti : ' , id_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica contenuto tabella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the vector\n",
    "cursor.execute('select * from  Vector_CV_PDF')\n",
    "#cursor.execute('select max(id_pdf) from  Vector_PDF')\n",
    "for row in cursor:\n",
    "     print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Richiesta al modello e conversione in embedding\n",
    "modificare la domanda a proprio piacimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = \"I've opened a new position for an Italian coding expert in Taleo application , in the meantime could you providing internal name showing me the highlights of this person\"\n",
    "response = co.embed(\n",
    "        texts=[prompt_question],\n",
    "        model='embed-english-v3.0',\n",
    "        input_type='search_query'\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricerca dei chunk nel vector db con il vettore KNN piu vicino "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39marray\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m----> 3\u001b[0m vector_prompt:List[\u001b[39mfloat\u001b[39m]\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39;49membeddings[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m cursor \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m      6\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mSELECT CHUNK  from Vector_CV_PDF where (ID_PDF, id_chunk) in \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m(\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mselect ID_PDF ,substr(ranking,instr(ranking,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,-1)+1)chunk_id from (\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m)\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39m)\u001b[39m\u001b[39m\"\"\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "import array\n",
    "from typing import List\n",
    "vector_prompt:List[float]=response.embeddings[0]\n",
    "\n",
    "cursor = connection.cursor()\n",
    "sql = \"\"\"SELECT CHUNK  from Vector_CV_PDF where (ID_PDF, id_chunk) in \n",
    "(\n",
    "select ID_PDF ,substr(ranking,instr(ranking,'-',-1)+1)chunk_id from (\n",
    "select ID_PDF , min(ranking) ranking\n",
    "  from (\n",
    "        select ID_PDF,VECTOR_DISTANCE(V,:1)||'--'||id_chunk ranking \n",
    "          from Vector_CV_PDF  \n",
    "        order by VECTOR_DISTANCE(V,:2)\n",
    "        )\n",
    "group by id_pdf order by 2,1 desc FETCH FIRST 3 ROWS ONLY\n",
    ")\n",
    ")\"\"\"\n",
    "\n",
    "sql = \"\"\"SELECT CHUNK  from Vector_CV_PDF where (ID_PDF, id_chunk) in \n",
    "(\n",
    "select ID_PDF ,substr(ranking,instr(ranking,'-',-1)+1)chunk_id from (\n",
    "select ID_PDF , min(ranking) ranking\n",
    "  from (\n",
    "        select ID_PDF,VECTOR_DISTANCE(V,:1)||'--'||id_chunk ranking \n",
    "          from Vector_CV_PDF  \n",
    "        order by VECTOR_DISTANCE(V,:2)\n",
    "        )\n",
    "group by id_pdf order by 2,1 desc FETCH FIRST 3 ROWS ONLY\n",
    ")\n",
    ")\"\"\"\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "        array_query1 = array.array(\"d\", vector_prompt)\n",
    "        array_query2 = array.array(\"d\", vector_prompt)\n",
    "        cursor.execute(sql,[array_query1,array_query1])\n",
    "        rows = cursor.fetchall()\n",
    "#print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione cohere per generare testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temp=0):\n",
    "  response = co.generate(\n",
    "    model='command',\n",
    "    prompt=prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=temp)\n",
    "  return response.generations[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera risposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kindly answer the following question  I've opened a new position for an Italian coding expert in Taleo application , in the meantime could you providing internal name showing me the highlights of this person based on  first_name :  Paolo Last Name:Cecchi Professional Summary: Experienced Cloud Interconnect Specialist with a proven track record of designing and implementing robust cloud connectivity solutions. and  first_name :  Eugenio Last Name: Bernaci Personal Summary:born in 1970 and is grow up in rome. and  first_name :  Mario Last Name: Bianchi  Professional Summary: Dynamic and results-oriented Enterprise Technology Architect with a wealth of experience in crafting and implementing cutting-edge solutions within the Oracle ecosystem. and  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, here are the internal names and role highlights for the new positions in Taleo application:\n",
      "\n",
      "1. Paolo Cecchi - Italian Coding Expert\n",
      "   - First name: Paolo\n",
      "   - Last name: Cecchi\n",
      "   - Professional Summary: Experienced Cloud Interconnect Specialist with a proven track record of designing and implementing robust cloud connectivity solutions in the Italian market. Paolo has extensive knowledge of network infrastructure and cloud computing, making him an ideal candidate to lead complex projects in the field of cloud interconnection.\n",
      "\n",
      "2. Eugenio Bernaci - Italian Coding Expert\n",
      "   - First name: Eugenio\n",
      "   - Last name: Bernaci\n",
      "   - Professional Summary: Seasoned Software Engineer with a strong background in cloud computing and software development. Eugenio's expertise in designing scalable and secure cloud architectures, as well as his proficiency in coding languages, positions him well to contribute to innovative projects within the Italian market.\n",
      "\n",
      "3. Mario Bianchi - Italian Coding Expert\n",
      "   - First name: Mario\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_prompt ='Kindly answer the following question  ' + prompt_question  + ' based on  ' ;\n",
    "for i in range(len(rows)):\n",
    "    text_prompt += rows[i][0]+' and  '\n",
    "\n",
    "#print (text_prompt)\n",
    "\n",
    "\n",
    "\n",
    "response = generate_text(text_prompt, temp=0.5)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
